{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_Head_Attention_Sub_Layer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXACkAtfNpG0"
      },
      "source": [
        "# The Attention Mechanism\n",
        "Copyright 2021, Denis Rothman, MIT License. Denis Rothman rewrote the reference notebook entirely in basic Python with no frameworks. Three more steps were added, and a Hugging Face transformer example was added. The original images were taken out, redesigned by Denis Rothman for educational purposes, and inserted in the book descriptions of the multi-attention sub-layer.\n",
        "\n",
        "The goal of this notebook is to obtain a mathematical view of the attention mechanism of transformer models. An industry 4.0 developer will become and AI expert with in-depth NLP knowledge. \n",
        "\n",
        "[The Reference Colaboratory Notebook was written by Manuel Romero](https://colab.research.google.com/drive/1rPk3ohrmVclqhH7uQ7qys4oznDdAhpzF)\n",
        "\n",
        "[A Medium article was written by Raimi Karim](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRoFjFRNXwJ"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLe9lWCJNogW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff872b45-fb1a-4c9b-d1c6-a23be59a2c48"
      },
      "source": [
        "print(\"Step 1: Input : 3 inputs, d_model=4\")\n",
        "x =np.array([[1.0, 0.0, 1.0, 0.0],   # Input 1\n",
        "             [0.0, 2.0, 0.0, 2.0],   # Input 2\n",
        "             [1.0, 1.0, 1.0, 1.0]])  # Input 3\n",
        "print(x)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Input : 3 inputs, d_model=4\n",
            "[[1. 0. 1. 0.]\n",
            " [0. 2. 0. 2.]\n",
            " [1. 1. 1. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZImwtHPN91V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7adf4dec-ba21-4cfe-a5e8-9a9ff882b7c2"
      },
      "source": [
        "print(\"Step 2: weights 3 dimensions x d_model=4\")\n",
        "print(\"w_query\")\n",
        "w_query =np.array([[1, 0, 1],\n",
        "                   [1, 0, 0],\n",
        "                   [0, 0, 1],\n",
        "                   [0, 1, 1]])\n",
        "print(w_query)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2: weights 3 dimensions x d_model=4\n",
            "w_query\n",
            "[[1 0 1]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kRBS7MUOFgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4d223b-f82d-42ea-da87-da47197c7446"
      },
      "source": [
        "print(\"w_key\")\n",
        "w_key =np.array([[0, 0, 1],\n",
        "                 [1, 1, 0],\n",
        "                 [0, 1, 0],\n",
        "                 [1, 1, 0]])\n",
        "print(w_key)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_key\n",
            "[[0 0 1]\n",
            " [1 1 0]\n",
            " [0 1 0]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Napm2VtkOIEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9e4418-cc84-46bd-c009-6cfa4bbd949c"
      },
      "source": [
        "print(\"w_value\")\n",
        "w_value = np.array([[0, 2, 0],\n",
        "                    [0, 3, 0],\n",
        "                    [1, 0, 3],\n",
        "                    [1, 1, 0]])\n",
        "print(w_value)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w_value\n",
            "[[0 2 0]\n",
            " [0 3 0]\n",
            " [1 0 3]\n",
            " [1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqapIgfDOQ7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06041946-1fbb-4900-a54d-2eaa799a3051"
      },
      "source": [
        "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
        "\n",
        "print(\"Queries: x * w_query\")\n",
        "Q=np.matmul(x,w_query)\n",
        "print(Q)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Matrix multiplication to obtain Q,K,V\n",
            "Queries: x * w_query\n",
            "[[1. 0. 2.]\n",
            " [2. 2. 2.]\n",
            " [2. 1. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmfMln1Wmv73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e75f47-f559-4043-d918-6303bdb33cc4"
      },
      "source": [
        "print(\"Step 3: Matrix multiplication to obtain Q,K,V\")\n",
        "\n",
        "print(\"Keys: x * w_key\")\n",
        "K=np.matmul(x,w_key)\n",
        "print(K)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3: Matrix multiplication to obtain Q,K,V\n",
            "Keys: x * w_key\n",
            "[[0. 1. 1.]\n",
            " [4. 4. 0.]\n",
            " [2. 3. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Asv-8mOWkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ee01da7-15d0-403d-95c5-ce757af0b352"
      },
      "source": [
        "print(\"Values: x * w_value\")\n",
        "V=np.matmul(x,w_value)\n",
        "print(V)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values: x * w_value\n",
            "[[1. 2. 3.]\n",
            " [2. 8. 0.]\n",
            " [2. 6. 3.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfgRAHUuOp5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08668ae2-ea63-4093-851c-e85417478595"
      },
      "source": [
        "print(\"Step 4: Scaled Attention Scores\")\n",
        "k_d=1   #square root of k_d=3 rounded down to 1 for this example\n",
        "attention_scores = (Q @ K.transpose())/k_d\n",
        "print(attention_scores)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4: Scaled Attention Scores\n",
            "[[ 2.  4.  4.]\n",
            " [ 4. 16. 12.]\n",
            " [ 4. 12. 10.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg2t6KuNOjzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb04d37f-e70d-4442-bc8d-91cdd3be960f"
      },
      "source": [
        "print(\"Step 5: Scaled softmax attention_scores for each vector\")\n",
        "attention_scores[0]=softmax(attention_scores[0])\n",
        "attention_scores[1]=softmax(attention_scores[1])\n",
        "attention_scores[2]=softmax(attention_scores[2])\n",
        "print(attention_scores[0])\n",
        "print(attention_scores[1])\n",
        "print(attention_scores[2])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 5: Scaled softmax attention_scores for each vector\n",
            "[0.06337894 0.46831053 0.46831053]\n",
            "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
            "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Es7A7NOvjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78d8072-c429-4214-dc44-7f246ff75bda"
      },
      "source": [
        "print(\"Step 6: attention value obtained by score1/k_d * V\")\n",
        "print(V[0])\n",
        "print(V[1])\n",
        "print(V[2])\n",
        "print(\"Attention 1\")\n",
        "attention1=attention_scores[0].reshape(-1,1)\n",
        "attention1=attention_scores[0][0]*V[0]\n",
        "print(attention1)\n",
        "\n",
        "print(\"Attention 2\")\n",
        "attention2=attention_scores[0][1]*V[1]\n",
        "print(attention2)\n",
        "\n",
        "print(\"Attention 3\")\n",
        "attention3=attention_scores[0][2]*V[2]\n",
        "print(attention3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 6: attention value obtained by score1/k_d * V\n",
            "[1. 2. 3.]\n",
            "[2. 8. 0.]\n",
            "[2. 6. 3.]\n",
            "Attention 1\n",
            "[0.06337894 0.12675788 0.19013681]\n",
            "Attention 2\n",
            "[0.93662106 3.74648425 0.        ]\n",
            "Attention 3\n",
            "[0.93662106 2.80986319 1.40493159]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBDKhaCvOzXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a4f2f33-cb1b-43ba-c315-b880c861da90"
      },
      "source": [
        "print(\"Step 7: summed the results to create the first line of the output matrix\")\n",
        "attention_input1=attention1+attention2+attention3\n",
        "print(attention_input1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 7: summed the results to create the first line of the output matrix\n",
            "[1.93662106 6.68310531 1.59506841]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjgRcqHO4ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db5e9ba-827b-4b69-ed54-e569faf58dbb"
      },
      "source": [
        "print(\"Step 8: Step 1 to 7 for inputs 1 to 3\")\n",
        "#We assume we have 3 results with learned weights (they were not trained in this example)\n",
        "#We assume we are implementing the original Transformer paper. We will have 3 results of 64 dimensions each\n",
        "attention_head1=np.random.random((3, 64))\n",
        "print(attention_head1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 8: Step 1 to 7 for inputs 1 to 3\n",
            "[[0.20216353 0.43707436 0.07130255 0.10509762 0.63655638 0.91845543\n",
            "  0.06511085 0.07839665 0.51316846 0.54571701 0.3358703  0.52092181\n",
            "  0.79275993 0.16141936 0.44289286 0.24053967 0.3109181  0.54028169\n",
            "  0.06520682 0.32269673 0.13930313 0.69075813 0.75375274 0.44131116\n",
            "  0.56119458 0.71839979 0.0468888  0.25340444 0.1991619  0.16644107\n",
            "  0.92459036 0.59387568 0.63877313 0.53316123 0.83347252 0.11372678\n",
            "  0.72663227 0.69516306 0.44438683 0.56273503 0.31350469 0.02059847\n",
            "  0.82681567 0.16856187 0.04148594 0.33847877 0.47074381 0.61881375\n",
            "  0.46843675 0.26956432 0.61479742 0.12607179 0.13802037 0.92223123\n",
            "  0.6391338  0.65150402 0.07907932 0.91520524 0.30195508 0.48254499\n",
            "  0.39480221 0.12666071 0.46255227 0.36424959]\n",
            " [0.0903881  0.29480922 0.85009611 0.17853338 0.34601251 0.93763393\n",
            "  0.540043   0.7225249  0.63869781 0.52115495 0.66330458 0.23289114\n",
            "  0.01969132 0.01413506 0.14770146 0.245892   0.99449404 0.79260813\n",
            "  0.83122168 0.61600548 0.58212976 0.70024249 0.70131622 0.47308278\n",
            "  0.00833893 0.78465381 0.15448052 0.4692497  0.35703875 0.36804557\n",
            "  0.63645655 0.03685476 0.43474866 0.16320524 0.74648885 0.0686991\n",
            "  0.49476582 0.63160464 0.63877061 0.66446338 0.29745341 0.80195157\n",
            "  0.78390113 0.01826606 0.36892904 0.49703574 0.83830997 0.7709826\n",
            "  0.5701037  0.57189029 0.58778716 0.63042531 0.50316925 0.00485138\n",
            "  0.15201332 0.28931219 0.66304687 0.18097074 0.58243657 0.15468443\n",
            "  0.21620307 0.61108016 0.85660662 0.97462751]\n",
            " [0.54035374 0.24429728 0.47934195 0.26930656 0.30767629 0.44342124\n",
            "  0.58670726 0.87973315 0.80297698 0.62699596 0.83420321 0.32667907\n",
            "  0.70524241 0.38973909 0.41884684 0.73826175 0.68025396 0.07994067\n",
            "  0.6963917  0.00317974 0.07195525 0.66279553 0.18599549 0.8415708\n",
            "  0.28805445 0.10571825 0.51767025 0.21372909 0.02995637 0.09740333\n",
            "  0.1432304  0.04585444 0.50804904 0.53577774 0.96570255 0.16875654\n",
            "  0.39413961 0.95761106 0.53840123 0.14185747 0.35669164 0.20888907\n",
            "  0.6096804  0.52207146 0.96209511 0.61025325 0.19761878 0.11649753\n",
            "  0.25276215 0.65990242 0.29377752 0.20611366 0.56685083 0.56316098\n",
            "  0.64042987 0.21198724 0.91560529 0.05537728 0.48451921 0.69681557\n",
            "  0.89377765 0.57734016 0.04606235 0.65252354]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI50dkZ1O630",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676f5c25-b484-488b-bcd8-800a763523bd"
      },
      "source": [
        "print(\"Step 9: We assume we have trained the 8 heads of the attention sub-layer\")\n",
        "z0h1=np.random.random((3, 64))\n",
        "z1h2=np.random.random((3, 64))\n",
        "z2h3=np.random.random((3, 64))\n",
        "z3h4=np.random.random((3, 64))\n",
        "z4h5=np.random.random((3, 64))\n",
        "z5h6=np.random.random((3, 64))\n",
        "z6h7=np.random.random((3, 64))\n",
        "z7h8=np.random.random((3, 64))\n",
        "print(\"shape of one head\",z0h1.shape,\"dimension of 8 heads\",64*8)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 9: We assume we have trained the 8 heads of the attention sub-layer\n",
            "shape of one head (3, 64) dimension of 8 heads 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n87LE92_Puf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0fe563-9d35-470a-e12f-05df996c1183"
      },
      "source": [
        "print(\"Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\")\n",
        "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
        "print(output_attention)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 10: Concatenation of heads 1 to 8 to obtain the original 8x64=512 output dimension of the model\n",
            "[[0.56468036 0.84093457 0.82947301 ... 0.38060399 0.40217666 0.41135804]\n",
            " [0.39462287 0.7404385  0.19385572 ... 0.35313777 0.09411459 0.34114942]\n",
            " [0.25418635 0.6578489  0.75116683 ... 0.88148612 0.91526444 0.80329105]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJLl4Jf3fPLh"
      },
      "source": [
        "And now with Hugging Face in one line!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZIRvcRmfTPb"
      },
      "source": [
        "#@title Transformer Installation\n",
        "!pip -q install transformers"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNwLYc-SfXdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4568a2-5b1c-499a-875f-ce6267b9b610"
      },
      "source": [
        "#@title Retrieve pipeline of modules and choose English to French translation\n",
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation_en_to_fr\")\n",
        "#One line of code!\n",
        "print(translator(\"It is easy to translate languages with transformers\", max_length=40))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base (https://huggingface.co/t5-base)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': \"Il est facile de traduire des langues Ã  l'aide de transformateurs\"}]\n"
          ]
        }
      ]
    }
  ]
}