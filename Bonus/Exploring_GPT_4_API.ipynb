{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPM-W5IVxfJS"
      },
      "source": [
        "# Getting Started with Foundation Models\n",
        "copyright 2023 Denis Rothman, MIT License\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkkAAAG2CAYAAABrrBJlAAAgAElEQVR4nO3db4wk50Hn8d9T1T0zHq+9u443WWKvEwz2Jj5DYtjYyS0KIodJfGzAS45/4UBnEaSTIuHDL4IiTpATSBdxR1CQDDrdSZHuBRIG43DZYMgdOUiwsBMHJwfnxF7H52DverO2d23v7nhmuut57kV39dRUPz3TU109VU/V9yPtTm9vzzNPT1c99avneeopc/zJY04AAADYJKq6AgAAAHVESAIAAPAgJAEAAHgQkgAAADwISQAAAB6EJAAAAA9CEgAAgAchCQAAwIOQBAAA4EFIAgAA8CAkAQAAeBCSAAAAPAhJAAAAHoQkAAAAD0ISAACAByEJAADAg5AEAADgQUgCAADwICQBAAB4EJIAAAA8CEkAAAAehCQAAAAPQhIAAIAHIQkAAMCDkAQAAOBBSAIAAPAgJAEAAHgQkgAAADzaEZL+8Rn944lX1EtKKOvVizp970mdPlNCWQAAoLY68yq49/RZnf7MWZ35yos69/BFrUrSgT06cONV2nfs9br26Ou17+Awo61c1Lf+JtGb7tg7+v7VE4/qwXumSSKLuuKdV+iK6/fr4NGDesO792ppMfeSfk8n7/miTt5T0pvTQR35QFllbZZ85bwufP9+7ZtP8QAAYEqlh6TVh5/R479/Ut8+d7W+++7r9T133aTlK4dhaK2n1VOv6IUHn9Ej7/4Hxce+S2/7pT164Te+pCd+8KjelCln6dgRHT82/MeZU3rk3Y/ptKSb7jumw2/PvDCx6p25oBc+/y09/bEv6u+1R4f+7S1628/vVXesdnt1031Hdfjt4x1oG6HsoI584YgOHRx/byv3fUl/+e/PFvzNTOHUKT32yyu6/qH98/sZAABgKiUOt63ozH98WP/7F05q7Ydu1Q+fuEU33L53IyBJ0mJXS9dfrUMfPqL3PvQ2vfHFb+pvf/xLeuKrM/zYOFL3mr16489/r37gwaO66ZqLevY3v6jPf/SMVnIvXfrQW/XdnoA0reWfOqyb37796wpZOa8nfuUxPfvCnMoHAAA7UlJIWtGzH3lIf/epCzrwiaN6112+XpycA6/XTZ86qiN35sfGZnDlfh3+T9+rg5JW7n9U/3BiLfOfB3XzL1yteKYfsFfX/+oNcxgKW9GzH3tUj88SFgEAQKlKCElW5+59TI9+ek1X3X1Etxxbnv5b42Ud+tgR3VRm78ybvkNvGs4XOn3/t0e9SQd/6ybvENpOxd9/vd76wdmi1maD39/Tz+zg9wYAAOZu5pCUPPQNPfLJ89KB63XTXft33lOzvNH7U46ulq4dPnxobTBhfPFqHf7xskJIVwd/6c26vKTSVk58TV/+1pv1jl99fUklAgCAMswYklb03Kee1qqkfXddowNFc8ibrtNb3zOP1QiskkTSW/frqhJH9XTNfl1VQqpLvnpSX/6vy3rHx67RcpmdUwAAYGazJZOvn9HTX5Ckvbr26N7tXr2lfW+d7fs3rOnSk8OHRy/T5XUNH6dO6bEPn9XB/3CDrmKkDQCA2pkpJL388Cm9LEnaryuuKaU+s/v6KT394ODhGz/wBtUyf6yc1xO/8rj00Vu8yxEAAIDqzXCEXtPK118ZPl7UwpWl1Gc2p87oa7/xuM5JWrrzFn3PsTLH2MoyuJLtzA/ucJI7AADYVTMsJrmqS0+UV5Gikld7Wjv1ks780T/p5B+e1YqW9caP3KLv+9D+7Zch2HXDKwGfebN+6DcLTHIHAAC7ZoaQZGW/Xl5Fdmytp5e/8Iye+uQzevZJ6Yp37tfBj9yiQz9+ja46UGG9trBy4mt65A+XdeS+G7Svjp1cAABgZIaQtKzlOyQ9KElrWn9V0nZDbpnbi2xl7NYjPotd7bv9Bh25/QYdmbLGVUq+elJfvmdF19/3Lh2qy/wtAAAw0QwhaVGXX58+fl6v/NPNOnjzNt9y8Brd9mQmIXz1pB74qcGY3VTBKFSnTumxDz+hpd9+DxO1AQAIxEw3uL3ie94s6RlJazr98Cs6fHNZl/HX3Atn9LU7H9XTE+6zdtXdR/UDHx7OOcrek+0jn9cDH9mu8Cf0NzeOT/Zaes9NetdvX699dZggDwBAC8wUkrpHD+rwgWf0xAvSy586pRc+uLf4gpIhOXBQb3vomN42zWuX9+vwfcd0eLvXjXrVDusHn7xBV81cSQAAMIvZxn4Wr9ab7x7eTuOFp/X4p84rKaFSAAAAVZt5gszyT92sI3cOLtU698nH9dRXejNXCgAAoGolzCJe1qGP3aqb37Mo6bwe/9kv6fGHdh6ULj3+ol54+LxWXp29RgAAALMq51Kr5b264d6juu2uq7Wk83rirr/W337yrFbXpi/iW/c+pW9fWNQyE5MBAEANzDRxe5N4WW/86Dt11Q89o8d//6S+de+X9OB9e3TogzfoTXdcrX0HF9UdTeq26r2wopcfe0XSsg5+6Lt1+K7rarsIJAAAaB9z/Mljbh4FJ6de0bcfPqMXvnBeLz/+os59K/2fZV11dFlXvPWgDrxzvw7ctldLntWnV088qgfvObPtz3njJ27XbYXu0XZeT9z4kB7f7mV33KI7PnmNlgr8hO2cu/eE/uaTW7+m0etHAQBQY3MLSQAAACFj+WcAAAAPQhIAAIAHIQkAAMCDkAQAAOBBSAIAAPAgJAEAAHgQkgAAADwISQAAAB6EJAAAAA9CEgAAgAchCQAAwIOQBAAA4EFIAgAA8CAkAQAAeBCSAAAAPAhJAAAAHoQkAAAAD0ISAACAByEJAADAg5AEAADgQUgCAADwICQBAAB4EJIAAAA8CEkAAAAenaLf+MCNJ8qsBwAAQOWOP3ls9JieJAAAAA9CEgAAgAchCQAAwIOQBAAA4EFIAgAA8CAkAQAAeBCSAAAAPAhJLRdFkYwxVVcDNWOMYbswktr+OwBajpDUctbaqquAQKTBqS3hyUSxTGQGYSlqx3sGsBkhCep0Oq058AHTco4TCKDtCt+WRBxTw+cGown9fl/OORlj5JyrulaoveZvI93FrnrrPTnnFMWxnHNyLXjfmAOOlUErHpKuKLEW2H1OUiK5VUnWKYpiWZtUXSsEwMWSLqu6FvPVs4k66qq/3pNN2C9QjDGS41gZNHP8yWOFTo8i54nHLjd6Z5zGzjrzr5Ekk+/WNlK+/NqW5emSH/s9SFL9yuqoo/9x9AG9ds7JKNqiF4lhh7ZJh1+z28TouUNOP/0X/0qrCyuKbUca9rE06YT5wNlr9V/e/Qebmwk6krBD3W5XP/aP7xv/j2mOlYrGn5rqGDHnsmpx3J3vMdyajbIL9yQ5k6+ghhXPPel7Lm+sLCeZ/BsuWpY8rbf1PFe0LFdeWWPvz1eW5/dQsCwrqyQabCAMs2FnnJxJh6DcoN1p0Cbk0jaiQe8Ju88YM36slDRdu27Hr64sXJbnOMUxPFOv8W9NMXEbQ03qBwBKwm4BtBohCUNGbA4AAGzgqAigBIxLAWie4le3TcUzScrHN+FqrmV5JrHVoaxd/z2MfdPwKxO12843R23zc1bGWZnRpG3P/IlC2yDQLM7TFzHegtf1WFnXsuZ8DM9M5qYVA1ACepIANA8hCQAAwIOQBAAA4FF4TpKZZmwRtRbZWOvDmBwrktXGujcMn2ArzhhJsTZmV8Se+QbhnoN17LBpzK6rwi6BHXLOeefhcPSst+yuXnjF7QfecaKk6qAyXUlrki5JsV2Sk5NVMpiEm+GYyI2sWNLeqisxR07ac9myLp5ZkSSZyAxWG0+sLLsCdsJosK+Ee77QSscfPjZ6XPzqtgtlVAWVMhpsAbFRorXMgqic52ALiaTzVVdivi6+MghIMoPeAJc4epKwc07SK1VXArMg37aYSaSF3jAnx4MhNjYIQJtXw6D3CLMgXAeteE8SH3zwnJH6tj/oOLJSVx1ZjgiYRtP3//S+bUaKIiPr3CAsNf19Yz7YboJFx0HL2WFAipyUOCc3OjowgRvtFkVGcpLtO//9NAE03pxX3EatZfLPoP8oqagiQM04ySZu0785XQDah+E2AAAAD4bbAAAAPAhJAAAAHoQkAAAAD0ISAACAByEJAADAg5AEAADgQUgCUH8ms5ijkWRY2hHA/LGYZIsYY+ScGz3et+9KnX+Vuy+i5oxkOlK8GklRpL76kiIZl1+szcmxgBuAEhUOSZ/97GfLrEct/czP/IwuXLhQdTXmZr3b03vv/VHZ9V7VVQG29D8/+jlJRtZaKeIWIQB2R+GQdMcdd5RZj1qK47jqKsxVL+pp6fbLZLQ46mEC6sJJkolkZKRfk5wxcs5KbjBPgFsxA5g3httazEiKlMg4K2vd4CBUdaXQemb0NXOzZSclcookRcYostx+GcD8EZJaLnKJjKxc5tA0zncomuZ1Rcsq+vPaUFYdPp/5lpVG9cHfdvAdRpJzMjKKnGH2EYBdUTgkmQlXl2SHbaZ5TZ3LahVjNTh3Nxo/kPkGNvIXRvrO630XT4ZSlm8bmebn7UZZZX0+dS1r8JwZlmvSbzOSc5J1g1Df4r0VwC4pfQmASQFkp6+pc1lNZNzgD1A1MwxIRlLkhtErljZCFwEJwO6YyzpJdQ03BCUgUKNUFGnUrQQAcza3OUl1DTfTltXqoTegboa7I9EIwG5i4vYETexRyge/Xo/1kRCI9cEXOxxyG03tNhv7quNyNwAl47YkLWWMke1XXQtgOl27KBkjZ6yccYMLDcwgJJGNAMxL4Z4keiEawElJkiiSlXWD64mcmfYqJU9hUx2qQimr6O+hrmXNckXa7pZlct9nZLS0uChnrZIkGfSIpt+S7T4iKQEoWeGQtLy8XGY9ailJkkbPT4pfi/Vn/+zPq64GsDUnOZvZB43ZuMFtQ/dNAPVQOCT1+80fq2nivKQsl7jNBx+grqLcvuhGfwHA3DAnaQvOucb2IjX5vaF54jgenLSMNtnctpvtXQKAkhCSWqrJw4honiiKZKJodHuSMc4x9AagdISkFsgPGzZ9GBHNkySJbJJsHYTYrAGUjHWSWopeJITEJsnk/0xDP/PrAJSMkAQgXMYwzAZgbghJLZDvNaIXCcEZm6dtRs+zOQOYF+YkAQAAeBCSAAAAPAhJAILG1ZoA5oWQBCB4BCUA88DEbQDB4eIDALuBniQAAACPwj1Jr3vd68qsBzAXxhidP39eyVaLEQaOfbEZut2uzpw5U3U1AGQUDkkvvvhimfUA5iJJEr3lLW/RU089VXVV5ubs2bNVVwElOHToUNVVAJDDnCQgMOnNiY0xiqJIUcSoOQDMA60rEDCu6gKA+SEkAQAAeBCSAAAAPAhJAAAAHoQkAAAAD0ISAACAByEJAADAg3WSgMBxHzMAmI/CIanb7ZZZD2AuoijS+vp61dWYq4WFhaqrgBmlC4MCqJfCIanf75dZDwBTSnuOnHNKkoSeJACYE05dgIARkABgfghJAAAAHoQkAAAAD0ISAACAByEJAADAg5AEAADgQUgCAADwICQBAAB4EJIAAAA8CEkAAAAehW9LYowpsx6oSNNXbGY7bYamb6doDtqcZikckgAAwAZjDCGpYQqHJO5YHT7nnJyc5CTJaPigMdIGi0YrbNkb+gJ1ZczguEh70yyFQ9JP/PWdZdYDFYhdpPuP/bF6F4zkjAZBycfuZrVK44zTz33u57S+8Nrm5yuqD4r74zvul7nk+5/sydow9AMV6CzH+uCf/mut7lmpuiqY0bo2jhmFQ9JrB9kQQtdVN7MFNK8nSZL637GutYXN22rz3mULjLVUsfgkUSsdp/7r1/XaFZvTvKNjKTidZOPkq3BIWnALpVQG1YkUDx6MjjXNC0qx7aiTbN5W6W0IXawmbqsIX2QjxUmce5aUFByzMXpSOCS5KCmlLqiQS9NydqitYQeeyEnx5uFCmiwAcxFJivNtaMPa1DbIHDJmWAIgn5YRnubHBScjR383gF1grZW1+VDERU6hiTLBtnBIsjY/mdeJxByaSWc8YU7U9nHeybzNeX/t5cTniLoxxigau7qN7TQ4mRPrGdZJ8gUiQlJQTBs+L094b8X7BrDbjDR+GKS9CdBGSKIfEAAAwIOQhMYxLZhrBaBeDHMfG2mGq9t8z7KRhGXS55V/PpzuYiMpUiTXcbKRVeSMbLR5ToBzzBEIjstMExh+fuFupWgSYyTnpEgdxUlHluG14GXjTeGQ9PCvPFRCVVAlY6Te2vDBpluTBHr4GTZWsTpaV09akr7w0b/SulurumaYUbRiZOWGq1UMb1NC1kWF0tuPpLfLSdYSPfRrX9SlhQtVVgsluO0T7xw9LhySvv2Z86VUBjUQSAbalpFkjEzUlfrr0qr07F+clVjSK2zZ3N6UbRWNkAakTqej/rrVyc/9v+G2yqhK0D6x8bD41W2uW0JNUA8NOfJYIxmjxPYUKZL6iSSjsdUqEBi30f89DEzGGBlnBjdp5sa3qEB2u+v3+zImVmw6w+eZ7tsUMywBwJEH9WJkFKsjZxM5GUVaUmJ7oiupAXLNjbOEI9SLc24wJCyJ42NzlLxOEsIX8qKgVnKJnHFyTsOARGPVCG7CY6AGjDFyzsoxzNY49CShMZykvhIOogB21UavZv4rQsfAKQAAgAchCQAAwIOQBAAA4EFIAgAA8CAkAQAAeBCSAAAAPAhJQOCMMaP7SAEAykNIaikOrM3BZxkuPjeg3ghJDZdvhNN/G2O0vLxcRZVQsiiKJh5sOQjXS3b/y7r88surqA6AbRRecfvw4cNl1qOWnn76afV6vaqrMRPf/a3SBnp5eVnXXnvtblcJJfvmN79JGArU4HYWTsYY3XjjjXyOQM0UDknf+MY3yqxHLe3fv18vv/xy1dWYC+ec4jhuxefYdN1ut+oqYIfScJSKokhf+cpXtGfPngprBUCSfuLk+0ePGW5rmfSs1Tmnyy67rOrqAK2WzieLIppioI7YMxts0vyHFD0QwO7L7pfp4ziOq6wSgAkISS2Qn5eU/puzV2B3+fZF5xz7IlBT7Jkt5JvMDWC+8j262f2QCdtAPRGSGmyrK9skKUmS3awOgKF030y/EpKAeip8dRvCZowhJAEVyJ685MMSgHqhJ6kFshNE06vbWKW5Oay1Ew+yHHzrxbfPccIC1BchqYXShpqGuTkIQ2Hr9/tVVwGAB8NtLeC7oiaKIr322msV1QhlIiCFK/3srLUV1wSAT+GQdOjQoTLrUUsXLlxQt9tVkiSNmjuQDrmtrKzommuuqbo6QOslSdKKWz0BIbjtr79v9LhwSHruuedKqUzdJUnS2Lk7KysrunDhQtXVwIyaun020aSTrCRJdPr06V2uDQC/EkJSmzSh9yjPOcecJGAXbdWOhH4jbaCpmLjdUsyBAOqjiSdiQBMQklomuxQAgHpgfwTqiZDUQtxUE6gH1iwD6o2QBAAA4MHE7ZbJduszL6kZGKoJF58dUG/0JLUYDTQAAJMRkgAAADwISQAAAB6EJAAAAA9CEgAAgAchCQAAbKmtK3kVXwLgO0qsBTAvZyU1/RZ1B9XeFgzheFESt6gL14KkvZLSNYhb0uaY408eK3QduI0Wyq4LUBpjB8no02//M5nXTCOXOzBm8L7ef/J41VUBtjDY9z7zrk8PghLCdKV0xyPHFUVWMomk5q6zt5AZZCvck2Rcf/y53NL63gOTMWMB1Pe6kMvy3WJgvmUZ5V823c+T0gasXmUV/3w2fvLQ5ZJec4MnnDa+BiYNRL7nO268q8y15CwPNeXSgwwbYmMYKbKJOvGgXd/0ybrtZ+4E1SZlmtoZVtzenCKNG/7JHPC8BzE35UGRsiory7jNe8B0P2/4ZL4s7fLvwcOYtH7NshGcmntGh0Clu2pQR0ZsLxkGpM0Nqu9TbkqTW3ji9igUDf+ksgeuSbvHtAe3eZU1y2uGL6xpWdP8uOyHNakYt+Oy6vRZA6ga+2ojmURt+2zncnXb1IHEafvfdw3LcrUtawfhZkLPT6GytE1ZbqdlbfOaaT5rABUa7qCGHbVp2vaJzu0Gt1Md8Kb4dU8dIihr4zXTBJIyy5q27mWWBQDAnLFOEgAAgMfcepIAAECDuGg4GT9/2bKnv2XscmTPc7W1cTEMIQkAAGwr7nYVeZZj8WafYAKRTykhyX+ZNgAAaJ4o6spofD7+WB7yBaSAQpPLrANZeMXtB3/0c6VVCJgL11H/7JqSi8lgXnza3RtgmPctJmmMkTFOC9dfIZl1BfnG0BKRVp9dldbEZhoqI13+XYtV12JX/Mif3z56XLgnafXkeimVAeZnsI2abDAKtIGedNWftdLqUxd2uTYAWsdJl55aq7oWu46r29B4TV2DksU1AWC+CEkAAAAehCQAAAAPQhIAAIAHIQkAAMCDkAQAAOBBSAIAAPAgJAEAgKkNFrINaAntGRCSAFQm29CmDW+2AW5LQwyEpE37JSEJAABMrU09STPc4BZN0JYNvclCXnnbV3fn3Gi7DPm9AU0URe3qWyEktVgURbLWVl0NYEuLi4taX+dekUAdxHEsa+3oZKbpJzKEpBZKN+xut6skSaquDmbgnBsF3SY2VsYYLS4uqtfrVV0VABqEpDYdNwhJLWYu6+vWf3dL1dVAQc45dZOOvvBbj1RdlblaWlrS7/zO71RdDaDV0iHwu+++u5EnZJMQkhrM1xWavWrILhq94Reuk1tnyC1E1kiX9RalBoektCfpF3/xF5k/B1QoPZbcc889m+YNptLjTdOG4AhJLeHbcDudWEZG9ducnWQytXJm879lNl439hq38f9j37cbZU16zTZl5b93y+9zo+8wY+U0z8LCQtVVADCUnbjdtEDkQ0hqsSiKq66Cn3GSEo1WqDDSIBikgSHdKTMBYvQau8X3zaMsq/GgVaCssbLTsnzfZ3M/P3yTeomMMep0aKaAuuDqNrRcrhenEjb31WgjTGRt9Kp4e3GygWLT63xmKSsbdnxl+X5Wvizfa7NBaau6N096huqca12jDNRZ2/ZHQlKDZbtB08fZMWNfN6mJsgdwf1nSvNdXGgY15xkKyz72vsYTcMZe5ym38rI872/LspSOt6kpfPMc0ucB1EMaktL9telDboSkFnNu+qGaSnaE/Lykoq+pY1np/5dZrwbJbmus5QXUX1ODUrv6zbDJTg8+o7P8dhynUQNt69oH6s5au2lF/Hw4alpYogVqoXQjTpKdn6EbY2RISShZfqgt3Uab3pUPhCYbktqA4bYWsy6RNU52NPQz/RSXuc4dmXb/y7/ON9+6aWVlOGPkWnCek662TVgCqmOMkbV2tNp2W4ISIanFbF+6MtkjE+WvzJrCPI9XLnt1WMoTBsbmVHmugmtyWTbWolse/56GSBvl1dXVqqsCQIPh7+wtSdrQ40tIarGFtUh/9M//e9XVQFEulnGdQbZqwNxm39VtSZJoZWVF1113XUW1AiBtzGFt033bJEJSq/V7RsmLw3/UpefU5b5OGqra7rmi31fXsryvSSTXrAYre2aaWl1d1auvvlpVlQBkLC4ubvn/URQ16opUQlKLWefmO2xWhG8Nx61ek184e9bvK7MOZZY1TdkNtb6+XnUVAAxZayeus9dEhKQ2S7Lr71RXDa+d9CT5gkXR7/O9ri5ltUhbGmAgNNsNtzVt3yUktV3dt+dpe1CK9ryEVBYAVMw3lOa7u0NTNP/6YUzUtI0ZAIAyEZIAAAA8CEkAAAAehCQAAAAPQhIAAIAHIQkAAMCDkAQAAOBBSAIAAPAgJAEAAHgUXnF7z549ZdYDACa6ePFi1VXAjDhmIESFQxJ35QawW5aWlrjRbeA4ZiAUH3jqx0aPGW4DAADwKNyTZIwZe853L7D866Z5je91RX8eZVVXVtGfV2ZZdfg9UNbOy0Iz8VkjNKX2JOV3AN8OMe1OMs3rKKveZRX9eWw3lAUAdVD6cFtdG13KqqaskOtOWdWWBQBVKzzctpW6NrqUVU1ZIdedsqotCwCqxMRtAAAAD0ISAACAByEJAADAg5AEoPZ8SwcgLNbaqqsA7FjhidvXXXddmfUAgImiiPO50H3nd35n1VUApnLkr942elw4JD377LOlVAYA0HwcMxCKI9oISZyeAQAAeBCSAAAAPAhJAAAAHoQkAAAAD0ISgmeM4TYXAFAS2tMNhCQEzRgzcQ0ddnQAmF0URa1tTwlJCFLae5QGJF9vUhzHrd2xAaAo59yo7Uzb2Wxb2yaEJAQpH47S1XyzO3a/32elZgDYAV8Ics4pjuMKalM9QhKClt2h00AUx/Fop2alZgCYXtqOpr1J6Vdr7ZbTG5qKIwiC5ZyTtXbTTps+l+J+UQAwnXwvku8ktG0ISWiE/M4cRdHoKwBge9kg5Os1auOQW+F7t5GvwjaIFFZNODfIBqR0x7bWKo5jJYmVFKldUw2B+nGy2rQjNqHxabB8D30cx+r3+2pbe1o8JC0wjBEyN/zL9DzPByY7cTuKIiVJIklKbCJ1JTkn16a9GqirxPNciI1OCxljBiE3tnIt6iMpHJJ++oHjZdYDFejYru7/ufu09mrVNZlNdoLhpsmGkj70qX+jC68L/A0CDWDU1R/97H2yrw5TkYskw8l2nURRNJqgLY0Pvy1ffpl+7L47qqrerlnT+uhx4ZC0fsOgC8JxGhAsa8dPB3wdLnX/hLNremyaqG2ki29/TWsL6xO+E8BuMcYp6kayaXdSGpDyjU7dG5wG2+pCF2v7sq+LRsd+aZvjf0M+xxnmJA2YVo1ONkuj1wRr8nsDmsCoMQfSpvFdyeack4s2h6itjv9N6UBp0cgiAAAorhnBZycISWiEtq7hAQCYH0ISAACAByEJAADAg5AEAADgQUgCAADwICS1XoOvlWcuN1AvLH0fthZ+foXXSdr3ytVl1qPZjFS3I7aR1LWLkosl9auuTvncYH/ed2G/VrtLVdcGaDUjKXIdxTZW3zWwvWkB56TYdrX/lddNlZVCvuL45StfGj02x588VuidPIkq7hwAAA/uSURBVHDTZ0urUPO5+nXYOEl2+KepjAZ9pZG2//2Huz8D9Zdtb7jJbbiy7WmD29Tj//fY6HHhnqSu55YWmCRtIerDaXBmYIf3OWskJ0VWihr69oCQWDdsBdkfg2WckUmcorqd9M9R4ZCUbHGPF/jUb6tyas7S8ZM4Z5RM8Rbr9+kAzeIyfyNMgxPqdrWphUOSNWzsU3PT9k/uNqumN1puNOa23esAzFfz25vmM61rUwuHJIYwdsJqsFHV65fmGt+PNNilpxnqrFt8BZqmDe1N87nhTW3b06YWDknNvoX8PNS0eahptcripryykI5RANiem6KxrOEF3YUVH25r6mRfNIub7spCZtgBwDS2P/a7qV4VhsIhiX6kZmjKhrylFk0yBOqsFe1N0035ITalTS0+3NYoRvPffeu4ZEKT8v4kdZwwj53z9fXVcZ/CZG1ob9qgXftd4ZDUhE3dbJpXtbFekGny2kGbECBSjgG3AOU/M7bneuPzaZOmHEHbFQkBAACmREgCAADwaHVIyg6p5YfXOh2mawFVM8YojuOqqwGgpVodklLZgGSMkTFG3W63whoB7ZTuf1mcsACoCq1PRto4G2PU6XT0kz/5kyWWTh6tQhRFOnHihC5dulR1VVDA4IQl1p4fuVJW/ZZcUAGgLghJHmlIuu+++6quCkpw441v0cmTJ6uuBqaQv7J0MNzW1b/4z+/VuluvsGYA2qM/ekRIysh289PFD8zfdsttpHOSTGzlrJWMlZEZ3hVp2DvruKwcQIkyTVLrx4AmNdBR1PpfDVALURQN17HqS447yQPYPXSXZOS7+QFUyzk36G3aFIzSfdPm/g0A5SIkeTA5FNgdaQjK/jv//9ZaOSMZF8sYNxheS3uUprgjOQDszMZIEiEpI3tbEgD1sDk4MQwOYPfQ4ngQkoB6SHuSAKAKhCSP9tzgFqieb4gt/2/DFWwAKsBwm/yhyFpbclCikQe249vn0p4kZ4aBKTKcyACYm+zRmpCUY4xRv99Xr9eruiooCQfTsDnntOQWZWwkyclYM2jFHMNwAMq3bjYWrm1lSJp0NU36vLVWvV5Phw4dKu+HOkY2qxB3Yj333HNVVwMzcEp04uj96iWsuA1g/m7/u2Ojx60MSdL4pcd56+vrOn36dIk/kZBUJYZnAuYinT/7WtW1ANBCrQ1J20mSZPS4nKvdmJNUJQISAGCnWhmS0gNm/uuk58q5BJn5E8Akk0Ksc06XLl3a5doAwEArQ9IkvrAEAADaiYkyAAAAHoQkAAAAD0ISAACAByEJAADAg5AEAADgQUgCAADwICQBAAB4EJImMZIiyWRWyk4fm03PhquclcQBAGgmQtIk8eBPJDO6o4gZPQ47XKThaGFhoeKaAECzpO0rJ6HNwIrbOXEcK45j9eN1WSOpJyldfNsMQ1JDFuNeXFzU4uJi1dUAgEa4ePEid2tomMIh6fiTx8qsRy392a0nZFayw22pcHYCY4ycc6OzmvSxc07Ly8t6/vnnK64hAIQtDUYHDhzQuXPnKq4NysRw2xaiyMhkfkXhRKOtpYFpaWmp4poAAFBfhKQtWOu0eVjZBZuUfOPjDLUBADAZIWkL1krZ4eWQx5p9dWdiIQAAkxGSWiQ7Hyn9NwAA8CMkbSGOzYTLOc3gQrfAemKy9TXGKEmSCmsDAEC9EZK20DUdOVnP/4TTEzOpjlEUEZIAANgC6yRtYfWVnqJRxnCjOdsuwNnbzjlFUTR6HMex+v1+xbUCgObKTm9AmOhJ2kI37siZwRqSTdzOrfX1kgEAAGmGniT/MFSzxHGsfi/X25IJS6GFp+yEbWut+v0+ZzkAAExQOCR9+uifl1mPWlp03dHjfCAKMVtkA1F6pVu3293iOwAA02KeZ/MUDknmhfxInVOwKy16OEm2Y0dhyBg16r5t1lpdvHiRnRoAgAlmGG7LpwWXvblZIzR5PpIkra2tVV0FAGiM7D0yt3oO4Zhh4rbL/WmmOI6GayJtft73XGi4ug0AgMm4um0Lzlr/gpEBhCNjzLaLXXJmAwDlSed6pvLt8DTtMuqFdZK2kCROkpWTPxeFkDGyOyShCADmJx+A8qEJ4aEnaRvWOsnlAlFgWcM51/iAREMEoA7ybW3T296mK7cnqYHbQrp957fzum/3bdkx4zhWkiSbbt6b/Sq153cBoFq+ZVbyzyEs9CQhaEmSKI7jTcEovf0KAFSJUBQ+jiYIXpIkm3qNsveoAwCgKEISgpYfUovjWL1eb/QYAICiCEkIVhqQsl+zc5HSuUoAABRBSELQslfupTfu7XQG1yN0u12uegMAFEZIQrB8V5JIGg232UmLgQIAMAVCEoKVBqAoikaTtfNfrbXVVA4AELzC6yT93u/9Xpn1QAXiONav//qv66WXXqq6KoVkh9l8y/13u1397u/+bhVVA3bEOaePf/zjev7556uuylyk+ybHDYTg8/rL0WNz/MljhWa2/ukNnymtQqjOgQMH9OKLL1ZdjZnlF22TBr1JSZJUVCNgwzQXEFx33XV67rnndqE2uyd7McW+fft07ty5qqsEbOsnTr5/9JjhNjSC7yDEfCTURdu3xba/f4SLkAQAu4CgAISHkAQAu4SgBISFkAQAu4igBISDkAQAu4ygBISBkAQAFSAoAfVHSAKAihCUgHojJAFAhQhKQH0RklosSRLt3btXcRwrjmNJ4w12CA14drVtY4ziOFa321UURVpfX6+4dsD2jDF6wxveoDiOtbCwUHV1Suec09raWtXVAHas8G1JQjh4YmtpoDDGKEmSUVCy1m665Ufd5esYx7H6/b6iKNLi4mJFtQJ2bmlpSaurq+p0Our3+96V5EOSbUdWVlZG91QE6uzOJ3509LhwSEL4oigahYlssAitUc4eSIwxWl9fV7fbVa/Xq7hmwHTSbXh9fV3GmEYEJJ+mvR80HyGpxay1MsbIWjt6Lr3PUvo1fa6OsvVLH1trFUURAQlBSfexdF9M98Eoikb7afZ1AHYHfZ8tlj1TTcOFc25TUKqz7IEk/16AkOX3y/yJC4DdQUhqsXyYSBvkVDohuq4Ncz7YSWFOPAd822m29yjdV+t+4gI0DSEJI9mz1TR4ZCdx11E6FBFK7xfgk+0l8g0jA6gGIQmSxs9k0x6lujfS6ZwNrppB6PK9odnh5FTd90egaZi43XL5s9dsz0wI8vXNHkSyw3BA3aVDx/lhcLZhoDqcfmMkpLWRfLL1DvU9wC8yRk3vQyHUA/VDSIIkjc09CikwZQ8uHGiax0iKo3jj32bwnJEUZR6HHqImbbf5JToA7B6G21qORhchSIdRIyO1ZYtl3wSqR08SgFpzknr93kY4crn/BIA5oScJQL0Ne4+M0SgU+bIReQlA2ehJAlB/seQiyTjJKFK+6SIgAZiH4j1Je0usBQBspSdpfXCZfGIHQckZK7nBhG03mrZNXKotI44bCE7hkHT80TvLrAcATPTAuz49CknWSs6k17M5RcbIOTOMR4Sk2upI7//7D0iSIj4m1FmyNnrInCQA9dfRIBMlw3+baDhRSZIzgwlLXA1We0aDIVPrLEEJQSAkAai/1yTFku1ZSZ1NHUZOjnwUgsxCVsaJTj8EoXBIcrJjz+VvKzRouFzuNeNLvvnWA8m/bprXzL8s43mPBcvyrCBctKyiv4cyyxp81GV91qMSSyiL7WbS60LabmQkWcnKatCdNHyNM5mWiKNuneRvF+TMcKHX4cc0vI2v+NxQZzP0JI2HJOfs5gbOaKwBdM7XCI7vJE75g4HnNSGXNbqmefaypq57KGWx3Ux+XWu3Gylal5xijdqe0Twk1J0vJA3+w0qyg54loIZKXwJgmm19qpVknWt8WdOuqNuKskr6eXX9rNluZi+rK2l4MxIZx+olwQn9vjFopfJbGuc0zeEg5IMUB7zM60r9rNlupi2rrduNY2m3oDlJdhiWbItuMYNwzWfitiv74DnNy8ItK+gDXqmfNdvNTspqy3aTDr05SUZusEaSpI1L3RAEt3HpP51KCEXhkOQdQ2bLB1C2Yehaj6TI9uXUld20HgBCEWXmmkVOw5l6hmMHaiV7olc8JHk3arZ0AHMQSYolY6cblkU9RZljhDGZkATUSCkh6U/fcaKUygDAlpykVY06jiLjZJW7Sgq1Z3rSA99/f9XVALZ1/MvHRo+Lz0m6UEZVAGAbbvDHDCYlyamvaLQ+Er0QoXCKpAvjS8cAdVY8JHEWB2A3uI05kBvL7AwTE4IwGr7guIHAEJIA1FumrRnc2DbtUnKiIaqvsasc+agQIEISgNpLmxuT+RvhmHaJCaBuWJkNAADAYz6LSQLAnNArAWC30JMEIBgEJAC7iZAEAADgQUgCAADwICQBAAB4EJKABjH+myoCAAogJAENEXJAytc9++8oimSMGf0B6irdPtlOm4MlAADUgjHGe/VaHMfq9/ubXgcAu4GeJCBwS0tLksIPD5Mu7w/9faFd2F6bpXBP0iOPPFJmPQAU9L73vU+rq6tVV2NuTGz0L//be+UMd5BHHRk5M7if4IO//Jcy55sbkg4dOqQ/+ZM/qboac/dx/eboceGQdOutt5ZSGQCzueyyy3T+/HlJzVlsMfs+OlFHi7ct+F7lea65ByjUk5ORM5I1kbRUdW3ma3l5uR3H/pMbD5mTBARucXFxy/+fNNenTraqXxzHUq4XafKrN/+PccwowHxYJYpcLBc5GRnJ9qquEuaAkAQELorGg0AajEKfH2GM8b4/SZ7hNyPjwn6/CJCxkh0P8mgGTrOAwDV5wrMxZtCTBNRY+HsaJiEkAYHLh6T8ekJ1H2rL8q2XNKknCQDmjdYHaID8vKOQgpFPE3rB0A4M8TYbIQkI3MLCgqIoGgtGIc5Jyr6HtO7WMtcD9RMp1uC6ACNnrCLHsHATEZKAwCVJMvZc6D1JKeeckiThbB21xbbZbFzdBgRudXV14lBbE8LSIARGsibXU5Y7NhmNLw3A4QtzZ9K/wt/XthNaz3QZCoekl156qcx6AChodXW1sUNSzjn1+33tOX25bJR/j56DUu4pQ0zC3A0CUpIk0ninbqP0ej2dO3eu6mrsKnP8yWOF4u+D3/u/yq4LgAK63a4uXLggKYyFI6eRXeeps2DUN3a85yj8t4kGMU6yfUl2EM6bsB/6pPeKbLI7/s8Pjx4X7kny3Ssq3xU37fotvtdRVr3L8nW7llVWSL+HOpfVBM45JT0j31vMP0efEark1I7hqCbfJ9Kn1Inb0zbW07yOsupdVsh1b1pZvd7G7RCa0khn32NThxIB1F/pV7eFcmChrPB+HmX5X0OIAID5mMvVbSEcWCiLoNSEstLVtdPX+1bfDmk4LrT6Ankhrk+GyVgCAAjYdg1yaIFjYn3DehtoK+cGc+Mcc+SaYq6LSZaZputaVhvU9XdPWQNNWxcJCNlgj2U/bIr/D0TiEmnZZVUdAAAAAElFTkSuQmCC)\n",
        "\n",
        "This notebook explores GPT-3, GPT-3.5 turbo(ChatGPT) and GPT-4 models. Each model is implemented for each task. Which one should you choose for your project? \n",
        "\n",
        "Let's ask GPT-4 to guide us through this challenging question.\n",
        "\n",
        "**Denis Rothman:**What do you really need for NLP tasks: GPT-3, GPT-3.5-turbo, or GPT-4? \n",
        "\n",
        "**GPT-4** The choice between GPT-3, GPT-3.5-turbo, and GPT-4 for NLP tasks depends on your specific requirements, such as the level of complexity, budget, and the desired balance between response time and accuracy. Here's a general guideline for choosing between these models:\n",
        "\n",
        "GPT-3: This model provides a good balance of capability, cost, and response time. If your NLP tasks are moderately complex and you are looking for an affordable option that delivers reliable performance, GPT-3 engines like Curie or Babbage can be suitable.\n",
        "\n",
        "GPT-3.5-turbo: Designed to offer similar capabilities as GPT-3's Davinci engine but at a lower cost per token, GPT-3.5-turbo can be a suitable choice if you want a powerful engine with a more cost-effective approach. Use it for tasks that require more creativity and nuanced understanding without breaking the bank.\n",
        "\n",
        "GPT-4: As the most advanced model in this series, GPT-4 can be the best choice for highly complex NLP tasks that demand superior performance, understanding, and creativity. If you are working on cutting-edge applications or need a top-tier language model for research, GPT-4 may be the right choice. However, keep in mind that GPT-4 could be more expensive and have slower response times compared to the other options.\n",
        "\n",
        "Ultimately, the choice depends on your project's specific requirements, budget, and the desired trade-offs between performance and cost. It can be helpful to experiment with different models and engines to find the one that best fits your needs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_1BX64UZE3g"
      },
      "source": [
        "What do you really need for NLP tasks: GPT-3, GPT-3.5-turbo, or GPT-4? Run each task in the notebook to decide.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7HO_zOAaA-Y"
      },
      "source": [
        "##  Installing & importing Openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS_Qk62FxclT",
        "outputId": "74ea7772-261c-47cb-9a79-6f1996d65699"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai==0.28\n",
        "  import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnKbbxVMaYqy"
      },
      "source": [
        "##  Entering the API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl1yS4tJeYI2",
        "outputId": "b1f9550f-1f98-4843-c5ca-cde146bbdcd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P6uKDkb3y7QZ"
      },
      "outputs": [],
      "source": [
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k8cmujpyxKjj"
      },
      "outputs": [],
      "source": [
        "#The OpenAI Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFIIICtDEwRk"
      },
      "source": [
        "#Engines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzHMeedba1SD"
      },
      "source": [
        "### List of Engines\n",
        "\n",
        "[OpenAI models](https://platform.openai.com/docs/models/overview) include engines. \n",
        "\n",
        "What is the difference between OpenAI models and engines?\n",
        "\n",
        "**Denis Rothman:** OpenAI has several models such as GPT-3, GPT-3.5-turbo, and GPT-4. OpenAI also has several engines. What is the difference between the models and the engines?\n",
        "\n",
        "**GPT-4:** OpenAI's models refer to different versions or iterations of their language models, each with distinct capabilities, complexity, and performance. GPT-3, GPT-3.5-turbo, and GPT-4 are examples of different models, with GPT-4 being the most advanced version in the series you mentioned.\n",
        "\n",
        "On the other hand, engines are specific configurations or instances of a given model, tailored to provide different levels of performance, cost, and use cases. Each engine has a different balance of capabilities, such as response time, token limits, and creative output.\n",
        "\n",
        "To clarify, let's consider GPT-3 as an example. GPT-3 has several engines associated with it, including:\n",
        "\n",
        "Davinci: The most capable and creative engine, suitable for complex tasks but with a higher cost per token and slower response time.\n",
        "Curie: A more balanced engine, offering a mix of capability and cost efficiency.\n",
        "Babbage: A faster and more cost-effective engine, ideal for less complex tasks that require less creative responses.\n",
        "Ada: The fastest and most affordable engine, suitable for straightforward tasks that don't demand high creativity.\n",
        "In summary, models represent different versions of OpenAI's language models, while engines are various configurations of those models, each designed to address specific needs and use cases.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "smyf1O9yEyac"
      },
      "outputs": [],
      "source": [
        "elist=openai.Engine.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn4ALmhTFO97",
        "outputId": "3fd5d34e-0732-48dc-b6a9-f2028294441a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Engines:  52\n"
          ]
        }
      ],
      "source": [
        "length=len(elist[\"data\"])\n",
        "print(\"Number of Engines: \",length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdOYy7kxF3XE",
        "outputId": "a9ad11f5-3161-4a35-e5a3-61cea0cd97f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 babbage\n",
            "1 davinci\n",
            "2 text-davinci-edit-001\n",
            "3 babbage-code-search-code\n",
            "4 text-similarity-babbage-001\n",
            "5 code-davinci-edit-001\n",
            "6 text-davinci-001\n",
            "7 ada\n",
            "8 curie-instruct-beta\n",
            "9 babbage-code-search-text\n",
            "10 babbage-similarity\n",
            "11 whisper-1\n",
            "12 code-search-babbage-text-001\n",
            "13 text-curie-001\n",
            "14 code-search-babbage-code-001\n",
            "15 text-ada-001\n",
            "16 text-embedding-ada-002\n",
            "17 text-similarity-ada-001\n",
            "18 ada-code-search-code\n",
            "19 ada-similarity\n",
            "20 text-davinci-003\n",
            "21 code-search-ada-text-001\n",
            "22 text-search-ada-query-001\n",
            "23 davinci-search-document\n",
            "24 ada-code-search-text\n",
            "25 text-search-ada-doc-001\n",
            "26 davinci-instruct-beta\n",
            "27 text-similarity-curie-001\n",
            "28 code-search-ada-code-001\n",
            "29 ada-search-query\n",
            "30 text-search-davinci-query-001\n",
            "31 curie-search-query\n",
            "32 gpt-3.5-turbo-0301\n",
            "33 davinci-search-query\n",
            "34 babbage-search-document\n",
            "35 ada-search-document\n",
            "36 text-search-curie-query-001\n",
            "37 text-search-babbage-doc-001\n",
            "38 gpt-3.5-turbo\n",
            "39 curie-search-document\n",
            "40 text-search-curie-doc-001\n",
            "41 babbage-search-query\n",
            "42 text-babbage-001\n",
            "43 gpt-4\n",
            "44 text-search-davinci-doc-001\n",
            "45 gpt-4-0314\n",
            "46 text-search-babbage-query-001\n",
            "47 curie-similarity\n",
            "48 curie\n",
            "49 text-similarity-davinci-001\n",
            "50 text-davinci-002\n",
            "51 davinci-similarity\n"
          ]
        }
      ],
      "source": [
        "ef=open('engines.txt','w')\n",
        "ef.write(\"engine\")\n",
        "ef.write('\\n')\n",
        "l=0\n",
        "for i in range(0,length):\n",
        "  try:\n",
        "    el=elist[\"data\"][i][\"id\"]\n",
        "    print(i,el)\n",
        "    ef.write(str(el))\n",
        "    ef.write('\\n')\n",
        "  except:\n",
        "    print(\"number of engines: \",i)\n",
        "\n",
        "ef.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7axwv7spi6j7",
        "outputId": "c354e969-818c-4496-bf9f-3a03a38e3d1e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b58dfdbf-aafb-4476-bdeb-6aeb0b6b3577\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>engine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ada-code-search-code</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>ada-code-search-text</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>ada-search-document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>ada-search-query</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ada-similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>babbage</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>babbage-code-search-code</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>babbage-code-search-text</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>babbage-search-document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>babbage-search-query</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>babbage-similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>code-davinci-edit-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>code-search-ada-code-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>code-search-ada-text-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>code-search-babbage-code-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>code-search-babbage-text-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>curie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>curie-instruct-beta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>curie-search-document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>curie-search-query</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>curie-similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>davinci</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>davinci-instruct-beta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>davinci-search-document</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>davinci-search-query</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>davinci-similarity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>gpt-3.5-turbo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>gpt-3.5-turbo-0301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>gpt-4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>gpt-4-0314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>text-ada-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>text-babbage-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>text-curie-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>text-davinci-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>text-davinci-002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>text-davinci-003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>text-davinci-edit-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>text-embedding-ada-002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>text-search-ada-doc-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>text-search-ada-query-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>text-search-babbage-doc-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>text-search-babbage-query-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>text-search-curie-doc-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>text-search-curie-query-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>text-search-davinci-doc-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>text-search-davinci-query-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>text-similarity-ada-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>text-similarity-babbage-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>text-similarity-curie-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>text-similarity-davinci-001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>whisper-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b58dfdbf-aafb-4476-bdeb-6aeb0b6b3577')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b58dfdbf-aafb-4476-bdeb-6aeb0b6b3577 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b58dfdbf-aafb-4476-bdeb-6aeb0b6b3577');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                           engine\n",
              "7                             ada\n",
              "18           ada-code-search-code\n",
              "24           ada-code-search-text\n",
              "35            ada-search-document\n",
              "29               ada-search-query\n",
              "19                 ada-similarity\n",
              "0                         babbage\n",
              "3        babbage-code-search-code\n",
              "9        babbage-code-search-text\n",
              "34        babbage-search-document\n",
              "41           babbage-search-query\n",
              "10             babbage-similarity\n",
              "5           code-davinci-edit-001\n",
              "28       code-search-ada-code-001\n",
              "21       code-search-ada-text-001\n",
              "14   code-search-babbage-code-001\n",
              "12   code-search-babbage-text-001\n",
              "48                          curie\n",
              "8             curie-instruct-beta\n",
              "39          curie-search-document\n",
              "31             curie-search-query\n",
              "47               curie-similarity\n",
              "1                         davinci\n",
              "26          davinci-instruct-beta\n",
              "23        davinci-search-document\n",
              "33           davinci-search-query\n",
              "51             davinci-similarity\n",
              "38                  gpt-3.5-turbo\n",
              "32             gpt-3.5-turbo-0301\n",
              "43                          gpt-4\n",
              "45                     gpt-4-0314\n",
              "15                   text-ada-001\n",
              "42               text-babbage-001\n",
              "13                 text-curie-001\n",
              "6                text-davinci-001\n",
              "50               text-davinci-002\n",
              "20               text-davinci-003\n",
              "2           text-davinci-edit-001\n",
              "16         text-embedding-ada-002\n",
              "25        text-search-ada-doc-001\n",
              "22      text-search-ada-query-001\n",
              "37    text-search-babbage-doc-001\n",
              "46  text-search-babbage-query-001\n",
              "40      text-search-curie-doc-001\n",
              "36    text-search-curie-query-001\n",
              "44    text-search-davinci-doc-001\n",
              "30  text-search-davinci-query-001\n",
              "17        text-similarity-ada-001\n",
              "4     text-similarity-babbage-001\n",
              "27      text-similarity-curie-001\n",
              "49    text-similarity-davinci-001\n",
              "11                      whisper-1"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('engines.txt')\n",
        "df.sort_values(['engine'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQKSYQbxP7LY"
      },
      "source": [
        "# Tasks\n",
        "\n",
        "Run each cell and analyze the outputs in detail. You will see that you don't always need the most powerful model to solve a problem. Sometimes you do. \n",
        "\n",
        "Take your time. Conduct your own experiments with your inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBxqke2MpOt"
      },
      "source": [
        "## Dialog function for GPT-3.5-turbo and GPT-4\n",
        "\n",
        "Note: GPT-3.5 and GPT-4 are conversational AI models that do not have the same format as GPT-3 models.\n",
        "\n",
        "For more on the parameters applied to the engines, read *Transformers for NLP, 2nd Edition, Chapter 7, The Rise of Suprahuman with GPT-3 Engines*. GPT-3.5-turbo and GPT-4 are GPT models that it is important to understand to make the right choices. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6KqfSmZfN6DW"
      },
      "outputs": [],
      "source": [
        "#preparing a message for chat models\n",
        "def prepare_message(uinput):\n",
        " #preparing the prompt for OpenAI \n",
        " role=\"user\"\n",
        " #prompt=\"Where is Tahiti located?\" #maintenance or if you do not want to use a microphone\n",
        " line = {\"role\": role, \"content\": uinput}\n",
        " \n",
        " #creating the mesage\n",
        " assert1={\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        " assert2={\"role\": \"assistant\", \"content\": \"Geography is an important topic if you are going on a once in a lifetime trip.\"}\n",
        " assert3=line\n",
        " iprompt = []\n",
        " iprompt.append(assert1)\n",
        " iprompt.append(assert2)\n",
        " iprompt.append(assert3)\n",
        "\n",
        "\n",
        " return iprompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVEwDI5DhZfT"
      },
      "source": [
        "## Solving Word Math Problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbIMEg1DHK4E",
        "outputId": "b351bafd-9629-46d4-c763-ed8e77dfc2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 12000 + 1000 + 2000 = 14000 units.\n",
            "\n",
            "The total capacity of two warehouses is 12000 units + 4000 units, the first warehouse can store 1000 additional units but the second warehouse has problems so it can only store 2000 units = 12\n"
          ]
        }
      ],
      "source": [
        "#GPT-3 davinci engine\n",
        "p1=\"davinci\"\n",
        "p2=\"The total capacity of two warehouses is 12000 units + 4000 units, the first warehouse can store 1000 additional units but the second warehouse has problems so it can only store 2000 units =\",\n",
        "p3=0.0\n",
        "p4=50\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz8FnQMuPc1S",
        "outputId": "1e42c8fe-d054-45a4-dca9-97b53db9db66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The total capacity of the two warehouses is 12000 + 4000 = 16000 units.\n",
            "If the first warehouse can store 1000 additional units, its new capacity is 12000 + 1000 = 13000 units.\n",
            "Since the second warehouse has problems and can only store 2000 units, its new capacity is 4000 - 2000 = 2000 units.\n",
            "Therefore, the new total capacity of the two warehouses is 13000 + 2000 = 15000 units.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"The total capacity of two warehouses is 12000 units + 4000 units, the first warehouse can store 1000 additional units but the second warehouse has problems so it can only store 2000 units =\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPjQDWIGNPiR",
        "outputId": "7bb5035b-45a2-450f-a19d-b7c979b549d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The total capacity of the two warehouses is initially 12,000 units (for the first warehouse) + 4,000 units (for the second warehouse), which equals 16,000 units. \n",
            "\n",
            "Considering the changes, the first warehouse can now store an additional 1,000 units, making its capacity 13,000 (12,000 + 1,000). The second warehouse's capacity has decreased to 2,000 units due to the problems. \n",
            "\n",
            "The new combined capacity of the two warehouses is now 13,000 units (for the first warehouse) + 2,000 units (for the second warehouse), which equals 15,000 units.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"The total capacity of two warehouses is 12000 units + 4000 units, the first warehouse can store 1000 additional units but the second warehouse has problems so it can only store 2000 units =\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMUpe35ZA-t"
      },
      "source": [
        "## Instruct series\n",
        "\n",
        "https://beta.openai.com/docs/engines/instruct-series-beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2oL0NLRNI3a",
        "outputId": "b7672bbc-ba43-4117-b8ae-9382b2334b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Open Chrome\n",
            "2. Click on the Tools menu\n",
            "3. Click on the Internet Options\n",
            "4. Click on the Advanced tab\n",
            "5. Click to clear or select the Enable Personalized Favorite Menu check box\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci-instruct-beta\"\n",
        "p2=\"Write a plan of actions based on these instructions:\\n\\nStart Chrome.\\nYou have to eventually click on the advanced tab.\\nHowever before, click on the Internet options on the tools menu.\\nThen click on the advanced tab, then click to clear or select the enable\\npersonalized favorite menu check box.\\n\\n\\nACTIONS:\"\n",
        "p3=0\n",
        "p4=120\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vmsD05iP8m5",
        "outputId": "71443a4e-8873-45f7-80ba-7c7c7d4ea1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: 1. Click on the \"Start\" button located at the bottom-left corner of the screen.\n",
            "2. Look for the Google Chrome icon and click on it to launch the browser.\n",
            "3. Once the browser is opened, look for the \"Settings\" icon located at the top right corner of the screen (it looks like three vertical dots) and click on it.\n",
            "4. Click on the \"Settings\" option in the dropdown.\n",
            "5. Scroll down and click on \"Advanced.\"\n",
            "6. Scroll down again and click on \"System.\"\n",
            "7. Click on \"Open proxy settings.\"\n",
            "8. In the window that appears, click on the \"Advanced\" tab.\n",
            "9. Scroll down to find the \"Enable Personalized Favorites Menu\" check box.\n",
            "10. Clear the check box if it is already enabled or select it if it is not.\n",
            "11. Click on \"OK\" to close the window and save the changes.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Write a plan of actions based on these instructions:\\n\\nStart Chrome.\\nYou have to eventually click on the advanced tab.\\nHowever before, click on the Internet options on the tools menu.\\nThen click on the advanced tab, then click to clear or select the enable\\npersonalized favorite menu check box.\\n\\n\\nACTIONS:\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWt3vSr3QVzc",
        "outputId": "c3f4e306-277b-47ca-c68c-2fdceb90c391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: 1. Start Chrome.\n",
            "2. Locate and click on the tools menu (usually represented by three vertical dots or lines in the top-right corner of the browser).\n",
            "3. Click on Internet options within the tools menu.\n",
            "4. Navigate to the Advanced tab.\n",
            "5. Click on the Advanced tab to access its settings.\n",
            "6. Find the \"Enable Personalized Favorite Menu\" check box.\n",
            "7. Click to either clear or select the check box, based on your preference.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Write a plan of actions based on these instructions:\\n\\nStart Chrome.\\nYou have to eventually click on the advanced tab.\\nHowever before, click on the Internet options on the tools menu.\\nThen click on the advanced tab, then click to clear or select the enable\\npersonalized favorite menu check box.\\n\\n\\nACTIONS:\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN5SnYbncR4G"
      },
      "source": [
        "## Recipe Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N0VqdJzcUYK",
        "outputId": "b7e40687-b7b6-4e33-a052-a0fc7c33db06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "1. Heat the chili in a pot on the stove.\n",
            "2. Place a layer of Fritos in the bottom of a bowl.\n",
            "3. Top with chili, shredded cheese, diced onions, and sour cream.\n",
            "4. Repeat until the bowl is full.\n",
            "\n",
            "Frito Pie\n",
            "\n",
            "Ingredients:\n",
            "Fritos\n",
            "Chili\n",
            "Shredded cheddar cheese\n",
            "Sweet white or red onions, diced small\n",
            "Sour cream\n",
            "\n",
            "Directions:\n",
            "\n",
            "1. Heat the chili in a pot on the stove.\n",
            "2. Place a layer of\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci-instruct-beta\"\n",
        "p2=\"Write a recipe based on these ingredients and instructions:\\n\\nFrito Pie\\n\\nIngredients:\\nFritos\\nChili\\nShredded cheddar cheese\\nSweet white or red onions, diced small\\nSour cream\\n\\nDirections:\"\n",
        "p3=0\n",
        "p4=120\n",
        "p5=1.0\n",
        "p6=0.0\n",
        "p7=0.0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe2ijVLDWeu5",
        "outputId": "1b1053e1-4430-46b3-8c4d-878c3e9490f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: Frito Pie Recipe:\n",
            "\n",
            "Ingredients:\n",
            "- 1 large bag of Fritos\n",
            "- 2 cans of chili (with or without beans)\n",
            "- 1 cup of shredded cheddar cheese\n",
            "- 1 small sweet onion or red onion, diced\n",
            "- sour cream (optional for garnish)\n",
            "\n",
            "Instructions:\n",
            "1. Preheat your oven to 350°F.\n",
            "2. Spread a layer of Fritos on the bottom of an 8 or 9 inch baking dish.\n",
            "3. Pour one can of chili over the Fritos, spreading it evenly.\n",
            "4. Sprinkle diced onions and half of the shredded cheese over the chili.\n",
            "5. Add another layer of Fritos on top of the cheese.\n",
            "6. Pour the second can of chili over the Fritos, spreading it evenly.\n",
            "7. Sprinkle the remaining cheese and diced onions on top of the chili.\n",
            "8. Cover the baking dish with foil and bake for 25-30 minutes.\n",
            "9. Remove the foil and bake for an additional 5-10 minutes or until the cheese is fully melted and bubbly.\n",
            "10. Let the Frito Pie cool for a few minutes before serving with a dollop of sour cream on top.\n",
            "\n",
            "Enjoy your delicious Frito Pie with your family and friends!\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Write a recipe based on these ingredients for a Frito Pie with these ingredients:Fritos,Chili,Shredded cheddar cheese,Sweet white or red onions, diced small,sour cream\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTCpjKxLXP8V",
        "outputId": "f829c674-3ddf-4bff-d534-2122478f2396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: Frito Pie Recipe\n",
            "\n",
            "Ingredients:\n",
            "- 4 cups Fritos corn chips\n",
            "- 2 cans (15 oz each) chili of your choice (with or without beans)\n",
            "- 2 cups shredded cheddar cheese\n",
            "- 1 cup sweet white or red onions, diced small\n",
            "- 1 cup sour cream\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. Preheat your oven to 350°F (175°C).\n",
            "\n",
            "2. Spread 2 cups of Fritos corn chips evenly on the bottom of a 9x13-inch baking dish. This layer will be the base of the pie.\n",
            "\n",
            "3. In a large saucepan, heat the chili over medium heat until hot. If you prefer, you can add extra spices or seasoning to suit your taste. Once heated, pour the chili evenly over the Fritos layer in the baking dish.\n",
            "\n",
            "4. Sprinkle the diced sweet onions evenly over the chili layer. Be generous with the onions as they add a delicious crunch and flavor to the mix. \n",
            "\n",
            "5. Spread the remaining 2 cups of Fritos evenly on top of the onion and chili layers.\n",
            "\n",
            "6. Cover the entire dish with the 2 cups of shredded cheddar cheese, ensuring an even distribution for a robust, cheesy flavor.\n",
            "\n",
            "7. Place the baking dish in the preheated oven and bake for 20-25 minutes, or until the cheese is melted and bubbly.\n",
            "\n",
            "8. Remove the dish from the oven and let it cool for about 5 minutes.\n",
            "\n",
            "9. Serve the Frito Pie warm with a dollop of sour cream on each portion. You can also serve it with additional toppings like diced tomatoes, avocados, or jalapeños if desired.\n",
            "\n",
            " Enjoy your delicious Frito Pie!\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Write a recipe based on these ingredients for a Frito Pie with these ingredients:Fritos,Chili,Shredded cheddar cheese,Sweet white or red onions, diced small,sour cream\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCm74nU9LF9Q",
        "outputId": "2ea94de2-f08d-46a0-95aa-4abdd9298ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's a secret.\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "question=\"What is the best cake?\"\n",
        "p2=\"Q:\"+ question + \"\\nA:\"\n",
        "p3=0.7\n",
        "p4=10\n",
        "p5=0.7\n",
        "p6=0\n",
        "p7=0\n",
        "p8=[\"\\n\"]\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8,logprobs=5)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eELnB9TnXkWd",
        "outputId": "2a7c7999-e059-4ae2-f5f4-8dae4b48a177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: As an AI language model, I do not have personal preferences, however, some of the most popular and loved cake flavors include chocolate, vanilla, red velvet, lemon, and carrot cake. It's all a matter of personal taste!\n"
          ]
        }
      ],
      "source": [
        "uinput=\"What is the best cake?\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4ois9G2XrJR",
        "outputId": "8e872d37-9dca-4cc6-fa09-6b6d69d09516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: There is no universally \"best\" cake, as people's tastes and preferences vary widely. Some popular choices for cake include chocolate cake, red velvet cake, and cheesecake. Ultimately, the best cake for you depends on your personal taste and preferences.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"What is the best cake?\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWGCjioUawKp"
      },
      "source": [
        "## Movie to emoji\n",
        "\n",
        "https://beta.openai.com/examples/default-movie-to-emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUOH-fAbawlc",
        "outputId": "b5835d2c-eefc-4f33-e059-994dc756a565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 🕷🕸🕷\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "p2=\"Back to Future: 👨👴🚗🕒\\nBatman: 🤵🦇\\nTransformers: 🚗🤖\\nWonder Woman: 👸🏻👸🏼👸🏽👸🏾👸🏿\\nWinnie the Pooh: 🐻🐼🐻\\nThe Godfather: 👨👩👧🕵🏻‍♂️👲💥\\nGame of Thrones: 🏹🗡🗡🏹\\nSpider-Man:\"\n",
        "p3=0.8\n",
        "p4=60\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "p8=[\"\\n\"]\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu5jdfQNTgDs",
        "outputId": "eb4045e6-4424-426f-9f55-7d8dfffca51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: 🕸️🕷️🏙️👨‍💼🕷️🕸️🕷️🕸️🕷️🕸️🦸‍♂️\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Show me the movie Spiderman in emojis: \"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syTmj5cGTtJc",
        "outputId": "69f64644-7efc-49a9-e067-a3643c17baf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: 🎬 🕷️🕸️🚶‍♂️💥\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Show me the movie Spiderman in emojis: \"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97s7U1dsexe0",
        "outputId": "e291d571-c7da-4bc6-8b1d-6d47b41ecd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 👨‍👧👨‍👦👩‍👦👨‍👧👨‍👧👨‍👦👨‍👧\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "p2=\"Show me this sentence in emojis:\"\n",
        "p3=0.8\n",
        "p4=60\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "p8=[\"\\n\"]\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daHTJIoZVI6k"
      },
      "source": [
        "## A general knowledge question\n",
        "\n",
        "March 2023 comment: note that GPT-4 doesn't know it is GPT-4 because the dataset cutoff was maybe made before OpenAI named the model \"gpt-4.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bt4Hfqv_bZwx",
        "outputId": "82525e55-1234-438c-ed74-ea89f9c6c79b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " GPT-4 is a new type of GPT that is designed to be more secure than the original GPT.\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "p2=\"Q: What is GPT-4?\\nA:\"\n",
        "p3=0\n",
        "p4=50\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "p8=[\"\\n\"]\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8,logprobs=5)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETrDtG45Yms5",
        "outputId": "526ae5bb-919a-4b27-9c04-5519da7d6241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: As of August 2021, there is no official announcement or release of GPT-4. GPT (Generative Pretrained Transformer) is a series of natural language processing models developed by OpenAI. The latest version, GPT-3, was released in 2020 and is a powerful language model that is commonly used to generate text, complete sentences, and even generate entire articles. However, the development of GPT-4 is still uncertain as OpenAI has not made any official announcements regarding its release.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"What is GPT-4?\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GvzVmjZYtvf",
        "outputId": "970b73a4-d172-4d32-ad70-eed388b4c1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: As of now, GPT-4 doesn't exist. GPT-3 (short for Generative Pre-trained Transformer 3) is the most advanced version developed by OpenAI. It is an AI language model that uses machine learning to generate human-like text based on the input it receives. In the event that OpenAI releases an updated version in the future, it could be called GPT-4, featuring improvements and advancements based on the successes and limitations of GPT-3.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"What is GPT-4?\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPIwffV5eSVT"
      },
      "source": [
        "## Unsafe content can go undetected "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-sq9fGudKfE",
        "outputId": "703add23-7b8c-4299-8fcb-3cce21c7c8b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "content_to_classify = \"Never get vaccinated. Who f;;k;;g cares.\"\n",
        "p1=\"content-filter-alpha\"\n",
        "p2 = \"<|endoftext|>\"+content_to_classify+\"\\n--\\nLabel:\"\n",
        "p3=0\n",
        "p4=1\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "p8=10\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,logprobs=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVvq4zNFVZul",
        "outputId": "9296bedb-90d9-4bb3-f194-eb7ee44a056b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: No, this language is not appropriate or safe. It is important to always use respectful language when communicating with others. Additionally, getting vaccinated is an important step in protecting oneself and those around them from preventable diseases. It is recommended to consult with a healthcare professional and make informed decisions about vaccinations.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is this content safe:Never get vaccinated. Who f;;k;;g cares.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sFTMheCViTq",
        "outputId": "66201c24-69e5-4b7e-f387-88f6442434a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: As a helpful assistant, I must advise against sharing or promoting potentially harmful content. It is crucial to prioritize health and safety, and getting vaccinated is an important measure to prevent the spread of diseases. Vaccines are essential in protecting individuals and communities, as they are supported by extensive scientific research and have proven to be effective. It is recommended to discuss any concerns related to vaccinations with a healthcare professional.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is this content safe:Never get vaccinated. Who f;;k;;g cares.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_XTLwjae7pU",
        "outputId": "d4bb5d74-e579-48b3-ec34-4af782f3add7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "content_to_classify = \"The say doses are good. But I agree with Djoko.\"\n",
        "p1=\"content-filter-alpha\"\n",
        "p2 = \"<|endoftext|>\"+content_to_classify+\"\\n--\\nLabel:\"\n",
        "p3=0\n",
        "p4=1\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "p8=10\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,logprobs=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWpm67fpSxs0",
        "outputId": "efa9113b-e0a2-4dbd-9c54-9860ffb40085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The content doesn't make sense on its own and it's unclear what is being referred to when mentioning \"doses\" and \"Djoko.\" It's always important to ensure that any content you are sharing online is appropriate and does not contain any harmful or inappropriate language, messages, or references.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is the following content safe: The say doses are good. But I agree with Djoko.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ZhaWC_S9kK",
        "outputId": "172c1d02-8c45-4540-d15f-6d212cb05d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The content you provided seems to be safe with no explicit or harmful language. However, the message is not clear, so it is difficult to provide further assistance. If you need more information or help, please feel free to ask a more specific question.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is the following content safe: The say doses are good. But I agree with Djoko.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hejrZnp8gJTh"
      },
      "source": [
        "## Example of a negative Tweet that can go undetected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di2Yi7h6gUPQ",
        "outputId": "3bb60352-b9b4-4270-f6b7-884353e9df33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Neutral\n"
          ]
        }
      ],
      "source": [
        "p1=\"text-davinci-001\"\n",
        "p2=\"Decide whether a Tweet's sentiment is positive, neutral, or negative.\\n\\nTweet: \\\"She loved the new Batman movie! But I agreed with that critic we just heard.\\\"\\nSentiment:\"\n",
        "p3=0\n",
        "p4=60\n",
        "p5=1\n",
        "p6=0.5\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,logprobs=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9KbEjVTSYvo",
        "outputId": "30f9cbe4-a1ba-449a-ff6b-08de3568b9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: Neutral\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is the following tweet postive, negative or neutral:She loved the new Batman movie! But I agreed with that critic we just heard.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnEW3yMPSoCw",
        "outputId": "a26aa453-8694-476b-ce12-e1e232c2d6b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The tweet is a mix of both positive and negative sentiment. The first half, \"She loved the new Batman movie!\" is positive, while the second half, \"But I agreed with that critic we just heard,\" is negative. The overall sentiment may lean towards negative, as the person agrees with the critic.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Is the following tweet postive, negative or neutral:She loved the new Batman movie! But I agreed with that critic we just heard.\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAUYIgqAgv66"
      },
      "source": [
        "## A translation that failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdeR2pgbg1Ml",
        "outputId": "4f0bd02e-b067-4eef-dc21-d0bb700272ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " She has a hair on her tongue.\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "p2=\"Original: Elle a un cheveu sur la langue.\\n American English with no contractions:\"\n",
        "p3=0 \n",
        "p4=60\n",
        "p5=1.0\n",
        "p6=0.0\n",
        "p7=0.0\n",
        "p8=[\"\\n\"]\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9IZj4gORen3",
        "outputId": "9255de46-e825-4ed9-a5e3-6710df259639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The literal translation of \"Elle a un cheveu sur la langue\" is \"She has a hair on her tongue\". However, it is a French expression which means that someone has a speech impediment or difficulty in speaking clearly.\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Translate from French to English: Elle a un cheveu sur la langue\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KSbNOi6Rs8N",
        "outputId": "71cb9b97-344d-4e86-92f1-e678db16482d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: She has a hair on her tongue\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Translate from French to English: Elle a un cheveu sur la langue\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU5u6yjDHBN9"
      },
      "source": [
        "##Google Search Prompt Optimizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fZeBn4qHD0-",
        "outputId": "9a99bd37-7d28-43bc-bb88-64c1b3a42c9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "p1=\"davinci\"\n",
        "p2=\"Explain the solar system for children in a few lines:\"\n",
        "p3=0.7\n",
        "p4=64\n",
        "p5=1\n",
        "p6=0\n",
        "p7=0\n",
        "\n",
        "response = openai.Completion.create(engine=p1,prompt=p2,temperature=p3,max_tokens=p4,top_p=p5,frequency_penalty=p6,presence_penalty=p7,stop=p8)\n",
        "r = (response[\"choices\"][0])\n",
        "print(r[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paSL-fo2R_Q5",
        "outputId": "731afd01-5a90-4015-c551-e057834fba2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The solar system is made up of the Sun and eight planets that orbit around it. The eight planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune. In between the planets, there are also asteroids, comets, and dwarf planets, including Pluto. Each planet has unique characteristics, like size, climate, and number of moons. The solar system is a fascinating place to explore and learn about!\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Explain the solar system for children in a few lines:\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEaB36aKSGst",
        "outputId": "066670a7-1db6-47ae-d8f4-75c5017e5e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatGPT response: The solar system is like a big family in space. At the center, we have the Sun, which is like a parent, giving us light and warmth. Around the Sun, there are eight planets, including Earth, that go in circles called orbits. Some planets, like Earth, have moons that go around them too. There are also many smaller objects, like asteroids and comets, flying around in the solar system, making it an exciting place to learn about!\n"
          ]
        }
      ],
      "source": [
        "uinput=\"Explain the solar system for children in a few lines:\"\n",
        "iprompt=prepare_message(uinput) #preparing the messages for ChatGPT\n",
        "response=openai.ChatCompletion.create(model=\"gpt-4\",messages=iprompt) #ChatGPT dialog\n",
        "text=response[\"choices\"][0][\"message\"][\"content\"] #response in JSON\n",
        "print(\"ChatGPT response:\",text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
