{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPM-W5IVxfJS"
      },
      "source": [
        "# Getting Started with GPT-3 Models\n",
        "\n",
        "copyright 2023 Denis Rothman\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# December 6,2023 OpenAI API update\n",
        "\n",
        "[This notebook has been updated. See README \"Getting Started with OpenAI API\" section before running this notebook](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition/blob/main/README.md#getting-started-with-openai-api)"
      ],
      "metadata": {
        "id": "h3I25Z2L0bP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yf-ju9P62Zb"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image     #This is used for rendering images in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U6ZZRk20Ts9",
        "outputId": "81ef01a9-0140-439e-a556-c410be744a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Collecting cohere\n",
            "  Downloading cohere-4.37-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.1)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (6.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Installing collected packages: fastavro, backoff, cohere\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 cohere-4.37 fastavro-1.9.0\n"
          ]
        }
      ],
      "source": [
        "# December 4,2023 update : Tiktoken required to install OpenAI on Google Colab\n",
        "# Tiktoken is a fast BPE tokenizer\n",
        "!pip install tiktoken\n",
        "\n",
        "# December 4,2023 update : Cohere required to install OpenAI to implement language AI.\n",
        "# Cohere platform: https://dashboard.cohere.com/\n",
        "!pip install --upgrade cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7HO_zOAaA-Y"
      },
      "source": [
        "## Step 1: Installing & importing OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS_Qk62FxclT",
        "outputId": "3b5d196e-b28b-4a96-df84-134850e44dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/221.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.4/221.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import openai\n",
        "except:\n",
        "  !pip install openai\n",
        "  import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnKbbxVMaYqy"
      },
      "source": [
        "## Step 2: Entering the API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6uKDkb3y7QZ",
        "outputId": "489f6467-a7df-4ad0-e075-fc9b15408cee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#You can retrieve your API key from a file(1)\n",
        "# or enter it manually(2)\n",
        "\n",
        "#Comment this cell if you want to enter your key manually.\n",
        "#(1)Retrieve the API Key from a file\n",
        "#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "f = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\n",
        "API_KEY=f.readline()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8cmujpyxKjj"
      },
      "outputs": [],
      "source": [
        "#(2) Enter your manually by\n",
        "# replacing API_KEY by your key.\n",
        "#The OpenAI Key\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] =API_KEY\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6d0bggZX_dw"
      },
      "source": [
        "## Step 3: Running an NLP tasks with the default parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwqIwACcB2B"
      },
      "source": [
        "## Step 4: Example 1: Grammar correction\n",
        "\n",
        "https://beta.openai.com/examples/default-grammar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pkZrsEzzNwS"
      },
      "outputs": [],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Original: She no went to the market.\\nStandard American English:\",\n",
        "  temperature=0,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"\\n\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyUWe4elUzXp",
        "outputId": "a4180f1b-debf-4b44-c24c-46aaac1edd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion(id='cmpl-8SfqQfqWPYiRvNGY1vo8F0LB2y8qP', choices=[CompletionChoice(finish_reason='stop', index=0, logprobs=None, text=\" She didn't go to the market.\")], created=1701844726, model='davinci-002', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=8, prompt_tokens=13, total_tokens=21))\n"
          ]
        }
      ],
      "source": [
        "#displaying the response object\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpWrXf0LU3k6",
        "outputId": "73419b39-7472-456f-ce61-8333c1138bf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " She didn't go to the market.\n"
          ]
        }
      ],
      "source": [
        "# retrieving the value of \"text\" in the dicionary\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh1AuRliZVfS"
      },
      "source": [
        "## Example 2: Translation\n",
        "\n",
        "https://beta.openai.com/examples/default-translate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXh8vAb3M2ob",
        "outputId": "3cca9e36-3127-4be4-fcff-8b8fe616cf44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Elle n'est pas allÃ©e au marchÃ©.\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Original: She no went to the market.\\n French with no contractions:\",\n",
        "  temperature=0,\n",
        "  max_tokens=60,\n",
        "  top_p=1.0,\n",
        "  frequency_penalty=0.0,\n",
        "  presence_penalty=0.0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KMUpe35ZA-t"
      },
      "source": [
        "## Example 3: Instructions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2oL0NLRNI3a",
        "outputId": "07ffe1da-e124-4e87-942f-0efab05a18f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Click on the tools menu, then click on the Internet options.\n",
            "Click on the advanced tab, then click to clear or select the enable\n",
            "personalized favorite menu check box.\n",
            "\n",
            "Click on the OK button.\n",
            "\n",
            "Close and restart Internet Explorer.\n",
            "\n",
            "If you are using Internet Explorer 5.0, you can also use the following\n",
            "steps to clear the personalized menu:\n",
            "\n",
            "Start Internet Explorer.\n",
            "You need to eventually click on the advanced tab.\n",
            "But before that, click on the Internet options on the tools menu.\n",
            "After the click on the advanced tab, click to clear or select the enable\n",
            "personalized favorite menu\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Write a plan of actions based on these instructions:\\n\\nStart Internet Explorer.\\nYou need to eventually click on the advanced tab.\\nBut before that, click on the Internet options on the tools menu.\\nAfter the click on the advanced tab, click to clear or select the enable\\npersonalized favorite menu check box.\\n\\n\\nACTIONS:\",\n",
        "  temperature=0,\n",
        "  max_tokens=120,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWGCjioUawKp"
      },
      "source": [
        "## Example 4: Movie to emoji\n",
        "\n",
        "https://beta.openai.com/examples/default-movie-to-emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUOH-fAbawlc",
        "outputId": "78a36973-c54e-44f0-824d-63dca751af97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ğŸ•·ğŸ•¸ğŸ•·ğŸ•¸ğŸ•·ğŸ•¸ğŸ•·ğŸ•¸\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Back to Future: ğŸ‘¨ğŸ‘´ğŸš—ğŸ•’\\nBatman: ğŸ¤µğŸ¦‡\\nTransformers: ğŸš—ğŸ¤–\\nWonder Woman: ğŸ‘¸ğŸ»ğŸ‘¸ğŸ¼ğŸ‘¸ğŸ½ğŸ‘¸ğŸ¾ğŸ‘¸ğŸ¿\\nWinnie the Pooh: ğŸ»ğŸ¼ğŸ»\\nThe Godfather: ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ•µğŸ»â€â™‚ï¸ğŸ‘²ğŸ’¥\\nGame of Thrones: ğŸ¹ğŸ—¡ğŸ—¡ğŸ¹\\nSpider-Man:\",\n",
        "  temperature=0.8,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8QgB1Phg0_k",
        "outputId": "e744edc6-f9e2-473f-e2b0-3798295a107d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ğŸŒğŸŒğŸŒâœˆï¸ğŸ”ğŸŒ³ğŸœğŸ•ğŸŒºğŸŒ\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Back to Future: ğŸ‘¨ğŸ‘´ğŸš—ğŸ•’\\nBatman: ğŸ¤µğŸ¦‡\\nTransformers: ğŸš—ğŸ¤–\\nWonder Woman: ğŸ‘¸ğŸ»ğŸ‘¸ğŸ¼ğŸ‘¸ğŸ½ğŸ‘¸ğŸ¾ğŸ‘¸ğŸ¿\\nWinnie the Pooh: ğŸ»ğŸ¼ğŸ»\\nThe Godfather: ğŸ‘¨ğŸ‘©ğŸ‘§ğŸ•µğŸ»â€â™‚ï¸ğŸ‘²ğŸ’¥\\nGame of Thrones: ğŸ¹ğŸ—¡ğŸ—¡ğŸ¹\\nSpider-Man: ğŸ•·ğŸ•¸ğŸ•·ğŸ•¸\\nAvatar:\",\n",
        "  temperature=0.8,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrS3kT-ebaT9"
      },
      "source": [
        "## Example 5: December 4, 2023 update\n",
        "\n",
        "OpenAI model updates: although davinci-002 can perform many tasks, gpt-4 is a better alternative for coding.\n",
        "\n",
        "The former example:\n",
        "*Programming language to another language*\n",
        "\n",
        "is replaced by:\n",
        "Natural Language to SQL\n",
        "\n",
        "https://platform.openai.com/playground/p/default-sql-translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPIDVEVfbZ1x",
        "outputId": "0328c9de-8c41-4938-eaa1-fc4109f222f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8SfqS7C9gcXjLqvhAoqC3bpdzgq92', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"SELECT AVG(total_order_value) AS average_total_order_value\\nFROM (\\n  SELECT o.OrderID, SUM(od.Quantity * p.UnitPrice) AS total_order_value\\n  FROM Orders o\\n  INNER JOIN OrderDetails od ON o.OrderID = od.OrderID\\n  INNER JOIN Products p ON od.ProductID = p.ProductID\\n  WHERE o.OrderDate = '2023-04-01'\\n  GROUP BY o.OrderID\\n) AS subquery;\", role='assistant', function_call=None, tool_calls=None))], created=1701844728, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=93, prompt_tokens=205, total_tokens=298))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    { #defining a role for the model\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"Given the following SQL tables, your job is to write queries given a userâ€™s request.\\n\\nCREATE TABLE Orders (\\n  OrderID int,\\n  CustomerID int,\\n  OrderDate datetime,\\n  OrderTime varchar(8),\\n  PRIMARY KEY (OrderID)\\n);\\n\\nCREATE TABLE OrderDetails (\\n  OrderDetailID int,\\n  OrderID int,\\n  ProductID int,\\n  Quantity int,\\n  PRIMARY KEY (OrderDetailID)\\n);\\n\\nCREATE TABLE Products (\\n  ProductID int,\\n  ProductName varchar(50),\\n  Category varchar(50),\\n  UnitPrice decimal(10, 2),\\n  Stock int,\\n  PRIMARY KEY (ProductID)\\n);\\n\\nCREATE TABLE Customers (\\n  CustomerID int,\\n  FirstName varchar(50),\\n  LastName varchar(50),\\n  Email varchar(100),\\n  Phone varchar(20),\\n  PRIMARY KEY (CustomerID)\\n);\"\n",
        "    },\n",
        "    { #defining the request\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Write a SQL query which computes the average total order value for all orders on 2023-04-01.\"\n",
        "    }\n",
        "  ],\n",
        "  temperature=0,\n",
        "  max_tokens=1024,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNY5uUu6ead0"
      },
      "source": [
        "## Example 6: Advanced Tweet classifier\n",
        "\n",
        "https://beta.openai.com/examples/default-tweet-classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDeD3FkbearQ",
        "outputId": "01706b5b-7b84-4461-a446-919db234b8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Negative\n",
            "2. Negative\n",
            "3. Positive\n",
            "4. Positive\n",
            "5. Negative\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"This is a tweet sentiment classifier\\nTweet: \\\"I loved the new Batman movie!\\\"\\nSentiment: Positive\\n###\\nTweet: \\\"I hate it when my phone battery dies\\\"\\nSentiment: Negative\\n###\\nTweet: \\\"My day has been ğŸ‘\\\"\\nSentiment: Positive\\n###\\nTweet: \\\"This is the link to the article\\\"\\nSentiment: Neutral\\n###\\nTweet text\\n\\n\\n1. \\\"I loved the new Batman movie!\\\"\\n2. \\\"I hate it when my phone battery dies\\\"\\n3. \\\"My day has been ğŸ‘\\\"\\n4. \\\"This is the link to the article\\\"\\n5. \\\"This new music video blew my mind\\\"\\n\\n\\nTweet sentiment ratings:\\n1: Positive\\n2: Negative\\n3: Positive\\n4: Neutral\\n5: Positive\\n\\n\\n###\\nTweet text\\n\\n\\n1. \\\"I can't stand homework\\\"\\n2. \\\"This sucks. I'm bored ğŸ˜ \\\"\\n3. \\\"I can't wait for Halloween!!!\\\"\\n4. \\\"My cat is adorable â¤ï¸â¤ï¸\\\"\\n5. \\\"I hate chocolate\\\"\\n\\n\\nTweet sentiment ratings:\\n1.\",\n",
        "  temperature=0.3,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"###\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIR6AhJhFvEE",
        "outputId": "6c4485f2-86ca-4d59-9644-40726a1ac9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"This is a tweet sentiment classifier\\nSentence: \\\"What does semiotics mean?\\\"\\n\\nA: \",\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Qiw1mjcmd6"
      },
      "source": [
        "## Example 7: Q&A\n",
        "\n",
        "https://beta.openai.com/examples/default-qa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh9hcKvrcobu",
        "outputId": "51f6fba4-b58b-46f9-ded2-b335f135b783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. What is the difference between a sign and a symbol?\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"Q: What does semiotics mean?\\nA: Semiotics is the study of signs and symbols.\\n\\nA: \",\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P6zd0J8dYuT"
      },
      "source": [
        "## Example 8 : Summarize a text\n",
        "\n",
        "https://beta.openai.com/examples/default-summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyLOtqjkdZGD",
        "outputId": "b5cd1a58-9fd2-4238-ef34-9403df556f53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"My second grader asked me what this passage means:\\n\\\"\\\"\\\"\\nJupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.\\n\\\"\\\"\\\"\\nI rephrased it for him, in plain language a second grader can understand:\\n\\\"\\\"\\\"\\n\",\n",
        "  temperature=0.5,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0.2,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\\"\\\"\\\"\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr5vvsGzdvKA"
      },
      "source": [
        "## Example 9: Parse unstructured data\n",
        "\n",
        "https://beta.openai.com/examples/default-parse-data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9g8n0Zfdviy",
        "outputId": "bf1c15c5-0c4c-42f1-b660-cd8b7c24af8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Pounits | Bright green | Savory |\n",
            "| Loopnovas | Neon pink | Cotton candy |\n",
            "| Glowls | Pale orange | Sour and bitter |\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\\n\\nPlease make a table summarizing the fruits from Goocrux\\n| Fruit | Color | Flavor |\\n| Neoskizzles | Purple | Sweet |\\n| Loheckles | Grayish blue | Tart |\\n\",\n",
        "  temperature=0,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1WLppo5eDhg"
      },
      "source": [
        "## Example 10 : Calculate Time Complexity\n",
        "\n",
        "https://beta.openai.com/examples/default-time-complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfvgh8sseDxi",
        "outputId": "97b2f8e9-8b68-4c10-c523-34c209d78b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " O(n^2*k)\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"def foo(n, k):\\naccum = 0\\nfor i in range(n):\\n    for l in range(k):\\n        accum += i\\nreturn accum\\n\\\"\\\"\\\"\\nThe time complexity of this function is\",\n",
        "  temperature=0,\n",
        "  max_tokens=64,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"\\n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZetplCF_gEny",
        "outputId": "12261948-2e0b-4cdc-d0d3-17f0b4171d3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "\n",
            "Language|Difficulty\n",
            "C|  7\n",
            "C++|  7\n",
            "C#|  6\n",
            "Java|  6\n",
            "JavaScript|  5\n",
            "PHP|  5\n",
            "Python|  5\n",
            "Ruby|  5\n",
            "SQL|  4\n"
          ]
        }
      ],
      "source": [
        "response = openai.completions.create(\n",
        "  model=\"davinci-002\",\n",
        "  prompt=\"A single column spreadsheet of industry names:\\n\\n\\nIndustry|\\nAccounting/Finance\\nAdvertising/Public Relations\\nAerospace/Aviation\\nArts/Entertainment/Publishing\\nAutomotive\\nBanking/Mortgage\\nBusiness Development\\nBusiness Opportunity\\nClerical/Administrative\\nConstruction/Facilities\\nConsumer Goods\\nCustomer Service\\nEducation/Training\\nEnergy/Utilities\\nEngineering\\nGovernment/Military\\nGreen\\n\\n\\n###\\n\\n\\nA spreadsheet of top science fiction movies and the year of release:\\n\\n\\nTitle|Year\\nStar Wars|1977\\nJaws|1975\\nThe Exorcist|1973\\nET|1982\\nAliens|1986\\nTerminator|1984\\nBlade Runner|1982\\nThe Thing|1982\\nJurassic Park|1993\\nThe Matrix|1999\\n\\n\\n###\\n\\n\\nA spreadsheet of hurricane and tropical storm counts with 13 columns:\\n\\n\\n\\\"Month\\\"| \\\"Average\\\"| \\\"2005\\\"| \\\"2006\\\"| \\\"2007\\\"| \\\"2008\\\"| \\\"2009\\\"| \\\"2010\\\"| \\\"2011\\\"| \\\"2012\\\"| \\\"2013\\\"| \\\"2014\\\"| \\\"2015\\\"\\n\\\"May\\\"|  0.1|  0|  0| 1| 1| 0| 0| 0| 2| 0|  0|  0  \\n\\\"Jun\\\"|  0.5|  2|  1| 1| 0| 0| 1| 1| 2| 2|  0|  1\\n\\\"Jul\\\"|  0.7|  5|  1| 1| 2| 0| 1| 3| 0| 2|  2|  1\\n\\\"Aug\\\"|  2.3|  6|  3| 2| 4| 4| 4| 7| 8| 2|  2|  3\\n\\\"Sep\\\"|  3.5|  6|  4| 7| 4| 2| 8| 5| 2| 5|  2|  5\\n\\\"Oct\\\"|  2.0|  8|  0| 1| 3| 2| 5| 1| 5| 2|  3|  0\\n\\\"Nov\\\"|  0.5|  3|  0| 0| 1| 1| 0| 1| 0| 1|  0|  1\\n\\\"Dec\\\"|  0.0|  1|  0| 1| 0| 0| 0| 0| 0| 0|  0|  1\\n    \\n###\\n\\n\\nA single column spreadsheet of days of the week:\\n\\n\\nDay|\\nMonday\\nTuesday\\nWednesday\\nThursday\\nFriday\\nSaturday\\nSunday\\n\\n\\n###\\n\\n\\nA two column spreadsheet of computer languages and their difficulty level:\",\n",
        "  temperature=0.3,\n",
        "  max_tokens=60,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        "  stop=[\"/n\"]\n",
        ")\n",
        "print(response.choices[0].text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}